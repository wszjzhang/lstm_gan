{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstmgan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMGAN(object):\n",
    "    \"\"\" LSTM-GAN implementation in tensorflow.\n",
    "\n",
    "        Args:\n",
    "            seq_size (int): Max size of sentence.\n",
    "            vocab_size (int): Size of words vocabulary.\n",
    "            first_input (int): Code of first word in generator.\n",
    "            hidden_size_gen (int, optional): Size of hidden layer\n",
    "                in generator.\n",
    "            hidden_size_disc (int, optional): Size of hidden layer\n",
    "                in discriminator.\n",
    "            input_noise_size (int, optional): Size of input noise\n",
    "                in generator input.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            dropout (float, optional): dropout rate.\n",
    "            lr (float, optional): learning rate in Adam optimizer.\n",
    "            grad_cap (float, optional): gradient cap value.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_size, vocab_size, first_input,\n",
    "                 hidden_size_gen = 512, hidden_size_disc = 512,\n",
    "                 input_noise_size = 32, batch_size = 128, dropout = 0.2,\n",
    "                 lr = 1e-4, grad_cap = 1.):\n",
    "\n",
    "        self.seq_size = seq_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size_gen = hidden_size_gen\n",
    "        self.hidden_size_disc = hidden_size_disc\n",
    "        self.input_noise_size = input_noise_size\n",
    "        self.batch_size = batch_size\n",
    "        self.first_input = first_input\n",
    "        self.keep_prob = 1 - dropout\n",
    "        self.lr = lr\n",
    "        self.grad_cap = grad_cap\n",
    "\n",
    "        self.build_model()\n",
    "        self.build_trainers()\n",
    "\n",
    "    def train_gen_on_batch(self, session, batch):\n",
    "        \"\"\"Train generator on given `batch` in current `session`\"\"\"\n",
    "        feed = {\n",
    "            self.input_noise: batch\n",
    "        }\n",
    "        ret_values = [self.gen_cost, self.gen_train]\n",
    "        cost, _ = session.run(ret_values, feed_dict = feed)\n",
    "        return cost\n",
    "\n",
    "    def train_disc_on_batch(self, session, noise_batch, real_batch):\n",
    "        \"\"\"Train discriminator on given `noise_batch` and `real_batch`\n",
    "        in current `session`\n",
    "\n",
    "        \"\"\"\n",
    "        feed = {\n",
    "            self.input_noise: noise_batch,\n",
    "            self.real_sent : real_batch\n",
    "        }\n",
    "        ret_values = [self.disc_cost, self.disc_train]\n",
    "        cost, _ = session.run(ret_values, feed_dict = feed)\n",
    "        return cost\n",
    "\n",
    "    def generate_sent(self, session, noise):\n",
    "        \"\"\"Generate one sentence in current `session` with given `noise`\"\"\"\n",
    "        feed_dict = {self.input_noise_one_sent: [noise]}\n",
    "        generated = session.run(self.sent_generator, feed_dict = feed_dict)\n",
    "        return np.argmax(generated[0], axis=1)\n",
    "\n",
    "    def build_model(self):\n",
    "        batch_size, input_noise_size, seq_size, vocab_size = \\\n",
    "            self.batch_size, self.input_noise_size, \\\n",
    "            self.seq_size, self.vocab_size\n",
    "\n",
    "        embedding = tf.diag(np.ones((vocab_size, ), dtype=np.float32))\n",
    "        self.embedding = embedding\n",
    "\n",
    "        input_noise = tf.placeholder(tf.float32, [batch_size, input_noise_size])\n",
    "        input_noise_one_sent = tf.placeholder(tf.float32, [1, input_noise_size])\n",
    "        self.input_noise = input_noise\n",
    "        self.input_noise_one_sent = input_noise_one_sent\n",
    "\n",
    "        real_sent = tf.placeholder(tf.int32, [batch_size, seq_size])\n",
    "        input_sentence = tf.nn.embedding_lookup(embedding, real_sent)\n",
    "        self.real_sent = real_sent\n",
    "\n",
    "        _, gen_vars = self.build_generator(input_noise, is_train = True)\n",
    "        generated_sent, _ = self.build_generator(input_noise, reuse = True)\n",
    "        sent_generator, _ = self.build_generator(input_noise_one_sent, reuse = True)\n",
    "        self.gen_vars = gen_vars\n",
    "        self.generated_sent = generated_sent\n",
    "        self.sent_generator = sent_generator\n",
    "\n",
    "        _, disc_vars = self.build_discriminator(input_sentence, is_train = True)\n",
    "        desc_decision_fake, _ = self.build_discriminator(generated_sent, reuse = True)\n",
    "        disc_decision_real, _ = self.build_discriminator(input_sentence, reuse = True)\n",
    "        self.disc_vars = disc_vars\n",
    "        self.desc_decision_fake = desc_decision_fake\n",
    "        self.disc_decision_real = disc_decision_real\n",
    "\n",
    "        self.gen_cost = 1. - desc_decision_fake\n",
    "        self.disc_cost = 1. - disc_decision_real*(1. - desc_decision_fake)\n",
    "\n",
    "\n",
    "    def build_trainers(self):\n",
    "        cap, lr, disc_cost, disc_vars, gen_cost, gen_vars = \\\n",
    "            self.grad_cap, self.lr, \\\n",
    "            self.disc_cost, self.disc_vars, \\\n",
    "            self.gen_cost, self.gen_vars\n",
    "\n",
    "        optimizer_disc = tf.train.AdamOptimizer(lr)\n",
    "        gvs = optimizer_disc.compute_gradients(disc_cost, disc_vars)\n",
    "        capped_grads_and_vars = [(tf.clip_by_value(grad, -cap, cap), var) \\\n",
    "                                 for grad, var in gvs]\n",
    "        optimizer_disc.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "        optimizer_gen = tf.train.AdamOptimizer(lr)\n",
    "        gvs = optimizer_gen.compute_gradients(gen_cost, gen_vars)\n",
    "        capped_grads_and_vars = [(tf.clip_by_value(grad, -cap, cap), var) \\\n",
    "                                 for grad, var in gvs]\n",
    "        optimizer_gen.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "        self.disc_train = optimizer_disc.minimize(disc_cost)\n",
    "        self.gen_train = optimizer_gen.minimize(gen_cost)\n",
    "\n",
    "    def build_generator(self, input_, reuse = False, is_train = False):\n",
    "        vocab_size, hidden_size_gen, input_noise_size, seq_size, keep_prob = \\\n",
    "            self.vocab_size, self.hidden_size_gen, \\\n",
    "            self.input_noise_size, self.seq_size, \\\n",
    "            self.keep_prob\n",
    "        embedding, first_input = self.embedding, self.first_input\n",
    "\n",
    "        with tf.variable_scope('generator_model', reuse = reuse):\n",
    "            input_noise_w = tf.get_variable(\n",
    "                \"input_noise_w\",\n",
    "                [input_noise_size, hidden_size_gen],\n",
    "                initializer=tf.random_normal_initializer(0, stddev=1/np.sqrt(vocab_size))\n",
    "            )\n",
    "            input_noise_b = tf.get_variable(\n",
    "                \"input_noise_b\",\n",
    "                [hidden_size_gen],\n",
    "                initializer=tf.constant_initializer(1e-4)\n",
    "            )\n",
    "            \n",
    "            first_hidden_state = tf.nn.relu(tf.matmul(input_, input_noise_w) + input_noise_b)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.GRUCell(hidden_size_gen)\n",
    "            if is_train:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "            input_w = tf.get_variable(\n",
    "                \"input_w\",\n",
    "                [vocab_size, hidden_size_gen],\n",
    "                initializer=tf.random_normal_initializer(0, stddev=1/np.sqrt(vocab_size))\n",
    "            )\n",
    "            input_b = tf.get_variable(\n",
    "                \"input_b\",\n",
    "                [hidden_size_gen],\n",
    "                initializer=tf.constant_initializer(1e-4)\n",
    "            )\n",
    "\n",
    "            softmax_w = tf.get_variable(\n",
    "                \"softmax_w\",\n",
    "                [hidden_size_gen, vocab_size],\n",
    "                initializer = tf.random_normal_initializer(0, stddev=1/np.sqrt(hidden_size_gen))\n",
    "            )\n",
    "            softmax_b = tf.get_variable(\n",
    "                \"softmax_b\",\n",
    "                [vocab_size],\n",
    "                initializer=tf.constant_initializer(1e-4)\n",
    "            )\n",
    "\n",
    "            state = first_hidden_state\n",
    "\n",
    "            labels = tf.fill([tf.shape(input_)[0], 1], tf.cast(first_input, tf.int32))\n",
    "            input_ = tf.nn.embedding_lookup(embedding, labels)\n",
    "\n",
    "            outputs = []\n",
    "            with tf.variable_scope(\"GRU_generator\"):\n",
    "                for time_step in range(seq_size):\n",
    "                    if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                    inp = tf.nn.relu(tf.matmul(input_[:, 0, :], input_w) + input_b)\n",
    "\n",
    "                    cell_output, state = cell(inp, state)\n",
    "                    logits = tf.nn.softmax(tf.matmul(cell_output, softmax_w) + softmax_b)\n",
    "                    labels = tf.expand_dims(tf.argmax(logits, 1), 1)\n",
    "                    input_ = tf.nn.embedding_lookup(embedding, labels)\n",
    "                    outputs.append(tf.expand_dims(logits, 1))\n",
    "\n",
    "            output = tf.concat(1, outputs)\n",
    "        variables = [v for v in tf.all_variables() if 'generator_model' in v.name]\n",
    "\n",
    "        return output, variables\n",
    "\n",
    "    def build_discriminator(self, input_, is_train = False, reuse = False):\n",
    "        vocab_size, hidden_size_disc, batch_size, seq_size,  keep_prob = \\\n",
    "            self.vocab_size, self.hidden_size_disc, \\\n",
    "            self.batch_size, self.seq_size, self.keep_prob\n",
    "\n",
    "        with tf.variable_scope('discriminator_model', reuse = reuse):\n",
    "            cell = tf.nn.rnn_cell.GRUCell(hidden_size_disc)\n",
    "            if is_train:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "            if is_train:\n",
    "                input_ = tf.nn.dropout(input_, keep_prob)\n",
    "\n",
    "            state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            input_w = tf.get_variable(\n",
    "                \"input_w\",\n",
    "                [vocab_size, hidden_size_disc],\n",
    "                initializer=tf.random_normal_initializer(0, stddev=1/np.sqrt(vocab_size))\n",
    "            )\n",
    "            input_b = tf.get_variable(\n",
    "                \"input_b\",\n",
    "                [hidden_size_disc],\n",
    "                initializer=tf.constant_initializer(1e-4)\n",
    "            )\n",
    "\n",
    "            with tf.variable_scope(\"GRU_discriminator\"):\n",
    "                for time_step in range(seq_size):\n",
    "                    if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                    inp = tf.nn.relu(tf.matmul(input_[:, time_step, :], input_w) + input_b)\n",
    "                    cell_output, state = cell(inp, state)\n",
    "\n",
    "            out_w = tf.get_variable(\n",
    "                \"discriminator_output_w\",\n",
    "                [hidden_size_disc, 1],\n",
    "                initializer=tf.random_normal_initializer(0, 1./np.sqrt(hidden_size_disc))\n",
    "            )\n",
    "            out_b = tf.get_variable(\n",
    "                \"discriminator_output_b\",\n",
    "                [1],\n",
    "                initializer=tf.constant_initializer(1e-4)\n",
    "            )\n",
    "\n",
    "            output = tf.reduce_mean(tf.sigmoid(tf.matmul(cell_output, out_w) + out_b))\n",
    "\n",
    "        variables = [v for v in tf.all_variables() if 'discriminator_model' in v.name]\n",
    "\n",
    "        return output, variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "#from lstm_gan import LSTMGAN\n",
    "\n",
    "DATASET_FILE = 'preprocessed.pickle'\n",
    "VOCABULARY_FILE = 'index_to_word.pickle'\n",
    "\n",
    "SENTENCE_SIZE = 30\n",
    "VOCABULARY_SIZE = 10000\n",
    "\n",
    "SENTENCE_START_TOKEN = \"START\"\n",
    "SENTENCE_END_TOKEN = \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self, disc_count=128, gen_count=10, data_path=\"data\", \n",
    "                batch_size=512, hid_gen=512, hid_disc=512, dropout=0.2,\n",
    "                grad_clip=0.1, noise_size=32, lr=1e-4):\n",
    "        self.disc_count = disc_count\n",
    "        self.gen_count = gen_count\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.hid_gen = hid_gen\n",
    "        self.hid_disc = hid_disc\n",
    "        self.dropout = dropout\n",
    "        self.grad_clip = grad_clip\n",
    "        self.noise_size = noise_size\n",
    "        self.lr = lr\n",
    "            \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use dataset with 268833 sentences\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.load_dataset(os.path.join(args.data_path, DATASET_FILE))\n",
    "index2word, word2index = utils.load_dicts(os.path.join(args.data_path, VOCABULARY_FILE))\n",
    "\n",
    "print(\"Use dataset with {} sentences\".format(dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c17d18a3bd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgrad_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seq_size, vocab_size, first_input, hidden_size_gen, hidden_size_disc, input_noise_size, batch_size, dropout, lr, grad_cap)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_trainers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mgenerated_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msent_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise_one_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m(self, input_, reuse, is_train)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'generator_model'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m       ops.convert_to_tensor(\n\u001b[1;32m   1177\u001b[0m           \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1180\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                          as_ref=False):\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 344\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "batch_size = args.batch_size\n",
    "noise_size = args.noise_size\n",
    "with tf.Graph().as_default(), tf.Session() as session:   \n",
    "    lstm_gan = LSTMGAN(\n",
    "        SENTENCE_SIZE,\n",
    "        VOCABULARY_SIZE,\n",
    "        word2index[SENTENCE_START_TOKEN],\n",
    "        hidden_size_gen = args.hid_gen,\n",
    "        hidden_size_disc = args.hid_disc,\n",
    "        input_noise_size = noise_size,\n",
    "        batch_size = batch_size,\n",
    "        dropout = args.dropout,\n",
    "        lr = args.lr,\n",
    "        grad_cap = args.grad_clip\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use dataset with 268833 sentences\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c0b026757600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mgrad_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seq_size, vocab_size, first_input, hidden_size_gen, hidden_size_disc, input_noise_size, batch_size, dropout, lr, grad_cap)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_trainers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mgenerated_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msent_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise_one_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b78c13f31706>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m(self, input_, reuse, is_train)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'generator_model'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m       ops.convert_to_tensor(\n\u001b[1;32m   1177\u001b[0m           \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1180\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                          as_ref=False):\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 344\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "#args = utils.get_args()\n",
    "dataset = utils.load_dataset(os.path.join(args.data_path, DATASET_FILE))\n",
    "index2word, word2index = utils.load_dicts(os.path.join(args.data_path, VOCABULARY_FILE))\n",
    "\n",
    "print(\"Use dataset with {} sentences\".format(dataset.shape[0]))\n",
    "\n",
    "batch_size = args.batch_size\n",
    "noise_size = args.noise_size\n",
    "with tf.Graph().as_default(), tf.Session() as session:   \n",
    "    lstm_gan = LSTMGAN(\n",
    "        SENTENCE_SIZE,\n",
    "        VOCABULARY_SIZE,\n",
    "        word2index[SENTENCE_START_TOKEN],\n",
    "        hidden_size_gen = args.hid_gen,\n",
    "        hidden_size_disc = args.hid_disc,\n",
    "        input_noise_size = noise_size,\n",
    "        batch_size = batch_size,\n",
    "        dropout = args.dropout,\n",
    "        lr = args.lr,\n",
    "        grad_cap = args.grad_clip\n",
    "    )\n",
    "\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    if args.save_model or args.load_model:\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    if args.load_model:\n",
    "        try:\n",
    "            saver.restore(session, utils.SAVER_FILE)\n",
    "        except ValueError:\n",
    "            print(\"Cant find model file\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    while True:\n",
    "        offset = 0.\n",
    "        for dataset_part in utils.iterate_over_dataset(dataset, batch_size*args.disc_count):\n",
    "            print(\"Start train discriminator wih offset {}...\".format(offset))\n",
    "            for ind, batch in enumerate(utils.iterate_over_dataset(dataset_part, batch_size)):\n",
    "                noise = np.random.random(size=(batch_size, noise_size))\n",
    "                cost = lstm_gan.train_disc_on_batch(session, noise, batch)\n",
    "                print(\"Processed {} sentences with train cost = {}\".format((ind+1)*batch_size, cost))\n",
    "\n",
    "            print(\"Start train generator...\")\n",
    "            for ind in range(args.gen_count):\n",
    "                noise = np.random.random(size=(batch_size, noise_size))\n",
    "                cost = lstm_gan.train_gen_on_batch(session, noise)\n",
    "                if args.gen_sent:\n",
    "                    sent = lstm_gan.generate_sent(session, np.random.random(size=(noise_size, )))\n",
    "                    print(' '.join(index2word[i] for i in sent))\n",
    "                print(\"Processed {} noise inputs with train cost {}\".format((ind+1)*batch_size, cost))\n",
    "\n",
    "            offset += batch_size*args.disc_count\n",
    "            if args.save_model:\n",
    "                saver.save(sess, utils.SAVER_FILE)\n",
    "                print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
